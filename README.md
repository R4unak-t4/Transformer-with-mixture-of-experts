# Transformer-with-mixture-of-experts
This repository implements a Transformer architecture enhanced with Mixture of Experts (MoE) layers. It uses a gating network with top-k routing and softmax masking to dynamically select experts, enabling efficient scaling and sparse activation for large models.
