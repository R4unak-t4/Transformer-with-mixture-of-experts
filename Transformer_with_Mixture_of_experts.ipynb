{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIpXWhU9+O0z6f07S4hTPd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQWJUw3ACQxT",
        "outputId": "ebc39f7a-1cf8-480b-cf7b-c7e74c9f041d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cb47211be10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeiLOVKaI4lU",
        "outputId": "526975d1-d091-4cea-9d4c-c5a470f19969"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-16 09:14:55--  https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-08-16 09:14:56 (27.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making the experts"
      ],
      "metadata": {
        "id": "hmGKqSpMK0dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "  '''\n",
        "  The expert can be seen as an neural network that expands the input embedding into 4 times it's dimention and re contracts it back to it's original dimention\n",
        "  '''\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4* n_embed), # take input and expand to 4 times the input dimenstion\n",
        "        nn.ReLU(), #we can also use GeLU\n",
        "        nn.Linear(4 * n_embed, n_embed), # contract it back to original dimension\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "IIVTv_saK0KD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we will need the routing matrix to route the tokens to the correct experts, this will help implement sparsity. After taking the dot product of the routing matrix and the original matrix we will get the expert selector matrix"
      ],
      "metadata": {
        "id": "FGOn1xeaML7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let:\n",
        "\n",
        "- $X \\in \\mathbb{R}^{B \\times D}$ : Input matrix (batch size $B$, input dimension $D$)  \n",
        "- $R \\in \\mathbb{R}^{D \\times E}$ : Routing (or gating) matrix (maps inputs to experts, with $E$ experts)  \n",
        "- $S \\in \\mathbb{R}^{B \\times E}$ : Expert selector matrix  \n",
        "\n",
        "The expert selector matrix is obtained as:\n",
        "\n",
        "$$\n",
        "S = X \\cdot R\n",
        "$$\n",
        "\n",
        "where:  \n",
        "\n",
        "- Each row of \\( S \\) corresponds to one input token’s affinity/score for each expert.  \n",
        "- A softmax is often applied along the expert dimension:\n",
        "\n",
        "$$\n",
        "\\hat{S}_{i,j} = \\frac{\\exp(S_{i,j})}{\\sum_{k=1}^{E} \\exp(S_{i,k})}\n",
        "$$\n",
        "\n",
        "to produce a probability distribution over experts.  \n",
        "\n",
        "Finally, the top-\\(k\\) experts are chosen per token:\n",
        "\n",
        "$$\n",
        "\\text{Experts}(i) = \\text{Top-}k(\\hat{S}_{i,:})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "sk31phq7M36_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load balancing :\n",
        "\n",
        "now we only select 2 experts for the tokens, Meaning we will take the top 2 values in the expert selector matrix and make every other value 0"
      ],
      "metadata": {
        "id": "JsOw_me3Ovie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now to we make he every other value negative infinity and apply softmax, this will result in every other value than top k values to be 0 and the top k values to have a sum of 1\n",
        "\n",
        "# Top-k Masking with Softmax\n",
        "\n",
        "Let $S \\in \\mathbb{R}^{B \\times E}$ be the expert score matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Top-k Masking\n",
        "For each token $i$, define a masked score matrix $M$ as:\n",
        "\n",
        "$$\n",
        "M_{i,j} =\n",
        "\\begin{cases}\n",
        "S_{i,j}, & \\text{if } j \\in \\text{Top-}k(S_{i,:}) \\\\[6pt]\n",
        "-\\infty, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Softmax over Masked Scores\n",
        "We then apply softmax:\n",
        "\n",
        "$$\n",
        "\\hat{S}_{i,j} = \\frac{\\exp(M_{i,j})}{\\sum_{m=1}^{E} \\exp(M_{i,m})}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why Non-Top-$k$ Entries Become Zero\n",
        "Since\n",
        "\n",
        "$$\n",
        "\\exp(-\\infty) = 0,\n",
        "$$\n",
        "\n",
        "all masked (non-top-$k$) entries vanish in the numerator and denominator.  \n",
        "\n",
        "Thus, only the top-$k$ entries survive, and their probabilities normalize to sum to 1.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Result\n",
        "- Non-top-$k$ experts: $\\hat{S}_{i,j} = 0$  \n",
        "- Top-$k$ experts: $\\hat{S}_{i,j} > 0$ and  \n",
        "\n",
        "$$\n",
        "\\sum_{j \\in \\text{Top-}k} \\hat{S}_{i,j} = 1\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "1IWQFzhyPsQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKRouter(nn.Module):\n",
        "  def __init__(self, n_embed, num_experts, top_k):\n",
        "    super(TopKRouter,self).__init__()\n",
        "    self.top_k = top_k\n",
        "    self.linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "  def forward(self, mh_output):\n",
        "    logits = self.linear(mh_output)\n",
        "    top_k_logits,indices = logits.topk(self.top_k, dim = -1)\n",
        "    zeros = torch.full_like(logits, float('-inf'))\n",
        "    sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "    router_output = F.softmax(sparse_logits, dim =-1)\n",
        "    return router_output,indices"
      ],
      "metadata": {
        "id": "c-oBVUGnR7uK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing noisy top-k gating"
      ],
      "metadata": {
        "id": "B8RbictmUc5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyTopKRouter(nn.Module):\n",
        "      def __init__(self, n_embed, num_experts, top_k):\n",
        "\n",
        "        super(NoisyTopKRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "      def forward(self, mh_output):\n",
        "        # Calculate the base logits for routing\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        # Calculate the noise logits\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "        # Add scaled unit gaussian noise to the logits\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "        # Get the top-k logits and their corresponding indices\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        # Create a sparse tensor with negative infinity for unselected experts\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        # Apply softmax to get the final routing probabilities\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "\n",
        "        return router_output, indices"
      ],
      "metadata": {
        "id": "XwXWr1OZTa-R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Mixture of Experts with sparsity\n",
        "\n",
        "* Every top k value from the expert selector matrxi nor get multipled with the corresponding top k experts.\n",
        "\n",
        "* But in practice we itterate over all the experts and get the values for the tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "vOSZLt8ZVyQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMoE(nn.Module):\n",
        "  def __init__(self,n_embed, num_experts, top_k):\n",
        "    super(SparseMoE,self).__init__()\n",
        "    self.router = NoisyTopKRouter(n_embed, num_experts, top_k)\n",
        "    self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    gating_output, indices = self.router(x)\n",
        "    final_output = torch.zeros_like(x)\n",
        "\n",
        "    flat_x = x.view(-1, x.size(-1))\n",
        "    flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "    for i,expert in enumerate(self.experts):\n",
        "      expert_mask = (indices == i).any(dim=-1)\n",
        "      flat_mask = expert_mask.view(-1)\n",
        "\n",
        "      if flat_mask.any():\n",
        "        expert_input = flat_x[flat_mask]\n",
        "        expert_output = expert(expert_input)\n",
        "\n",
        "        gating_scores = flat_gating_output[flat_mask,i].unsqueeze(1)\n",
        "        weighted_output = expert_output * gating_scores\n",
        "\n",
        "        final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "t8qgu1MUVSeW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Complete MoE"
      ],
      "metadata": {
        "id": "oW0fWz_JaACY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "  '''\n",
        "  The expert can be seen as an neural network that expands the input embedding into 4 times it's dimention and re contracts it back to it's original dimention\n",
        "  '''\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, 4* n_embed), # take input and expand to 4 times the input dimenstion\n",
        "        nn.ReLU(), #we can also use GeLU\n",
        "        nn.Linear(4 * n_embed, n_embed), # contract it back to original dimension\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class NoisyTopKRouter(nn.Module):\n",
        "      def __init__(self, n_embed, num_experts, top_k):\n",
        "\n",
        "        super(NoisyTopKRouter, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
        "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
        "\n",
        "      def forward(self, mh_output):\n",
        "        logits = self.topkroute_linear(mh_output)\n",
        "        noise_logits = self.noise_linear(mh_output)\n",
        "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
        "        noisy_logits = logits + noise\n",
        "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
        "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
        "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
        "        router_output = F.softmax(sparse_logits, dim=-1)\n",
        "\n",
        "        return router_output, indices\n",
        "\n",
        "class SparseMoE(nn.Module):\n",
        "  def __init__(self,n_embed, num_experts, top_k):\n",
        "    super(SparseMoE,self).__init__()\n",
        "    self.router = NoisyTopKRouter(n_embed, num_experts, top_k)\n",
        "    self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
        "    self.top_k = top_k\n",
        "\n",
        "  def forward(self, x):\n",
        "    gating_output, indices = self.router(x)\n",
        "    final_output = torch.zeros_like(x)\n",
        "\n",
        "    flat_x = x.view(-1, x.size(-1))\n",
        "    flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
        "\n",
        "    for i,expert in enumerate(self.experts):\n",
        "      expert_mask = (indices == i).any(dim=-1)\n",
        "      flat_mask = expert_mask.view(-1)\n",
        "\n",
        "      if flat_mask.any():\n",
        "        expert_input = flat_x[flat_mask]\n",
        "        expert_output = expert(expert_input)\n",
        "\n",
        "        gating_scores = flat_gating_output[flat_mask,i].unsqueeze(1)\n",
        "        weighted_output = expert_output * gating_scores\n",
        "\n",
        "        final_output[expert_mask] += weighted_output.squeeze(1)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "SI-XcgGMZZlm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer block"
      ],
      "metadata": {
        "id": "EAgvL51Lagvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "qtXnpurOaFmy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embed, n_head, num_experts, top_k):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.smoe(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "SUHWP4FJbEP3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseMoELanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head, num_experts=num_experts, top_k=top_k) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "          idx_cond = idx[:, -block_size:]\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "      return idx"
      ],
      "metadata": {
        "id": "C5cY2JsrbRXb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(67)\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "DO4mUg0OdFu6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "zIFjGBj7e-q4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyper parameters"
      ],
      "metadata": {
        "id": "ksexK5vcfcNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 20\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "head_size = 16\n",
        "n_embed = 128\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "num_experts = 8\n",
        "top_k = 2"
      ],
      "metadata": {
        "id": "yFWz_atIfJk8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import init\n",
        "def kaiming_init_weights(m):\n",
        "  if isinstance(m,(nn.Linear)):\n",
        "    nn.init.kaiming_normal_(m.weight)"
      ],
      "metadata": {
        "id": "WSuG4Ei4fabc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SparseMoELanguageModel()\n",
        "model.apply(kaiming_init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U29zrdF8f5FC",
        "outputId": "9a8831fb-860f-4fc2-b7ec-eae5a1eb28ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseMoELanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(32, 128)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-7): 8 x Head(\n",
              "            (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (smoe): SparseMoE(\n",
              "        (router): NoisyTopKRouter(\n",
              "          (topkroute_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "          (noise_linear): Linear(in_features=128, out_features=8, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-7): 8 x Expert(\n",
              "            (net): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (1): ReLU()\n",
              "              (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (3): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e9Tyhs-f-Zk",
        "outputId": "a9ccf3a5-e7cb-4a7e-b1ea-b36072bf7bb3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.996545 M parameters\n",
            "step 0: train loss 5.1506, val loss 5.1520\n",
            "step 19: train loss 3.3006, val loss 3.3420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype =torch.long, device=device)\n",
        "print(decode(m.generate(context,max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ4UZLKQhP76",
        "outputId": "257fad60-b0f0-4afe-f23c-450abfd3f08b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "uGhto  n  wnyE GeaS gYAb dfdtnirten,\n",
            "nE tBusrQhUntksicytoo\n",
            "\n",
            "cadobEn ou  e giirfdgxnqcyr rdtrt t  y Oase a?L,kd.orn t tg wo  p uouasa H ,hh  r  n,yu \n",
            "eT\n",
            "o o,h ws ,spy. bai n ioCIIpr dchyt\n",
            " hhetGeVrrBue  tdf Eayiooottto hdAI an  i, AokoaomsTBitwe? onasbwh eA, c    r,muaeTnrr Ido mt:tm e\n",
            "tn   eo dr a; oMnt c a me rtainoAtstoo  whrbC meB:tyhusng uI e ,ptIswaroro L mr sUsg :rooe'sn tsdagwr ,rItneItulAa h\n",
            "hin m,e ntWr I Urht: H sts.\n",
            ",  tgres  ohonvaa aeduadt tKno y a\n",
            "ioi I ue,t in Bo m  ieCTT\n",
            "fe  I,If\n",
            "n i teil!wI  nt io$tiima'a ss\n",
            "ola trcdo ld tchola  rs  rt\n",
            "uw ,en,s d,thiIho rib i\n",
            "M o-. hd: nliu\n",
            "  hhs\n",
            "hiu woe m \n",
            "iidm  t ,anhniq Krwlr.r r rp tycNb e ha in,e H,tkqeBd :yth srro hhatt Cu tocrnbo,Wi ynhl: donoun i\n",
            "-uu,ah;r tEEiH hngdtmi   eio nIgnohs oorrtsip\n",
            "Eestoidtdotttsmrrru ,,InUtlt MaPcsyoaehI mNehauss i,,si.nu\n",
            "lahue m rB rt\n",
            " :anesaee s.edofTtira  art nirdee  nrrDrob Im,aseUrs e se\n",
            "iut dgfs ap du:e,in wlynw tIddntdtiag   ns r nlti r al m  gym tn\n",
            "r asntls.h ,ia. od  rtenpea mtoOgtyddrsi  cobO aiI;mbd th.o p  ,t Tltf\n",
            "aeg, oi; \n",
            "a,yhdhYoasforni ya:h , \n",
            " Iht Z\n",
            "its  fthydnarBwA , ylnr   nenatd yoeoreat  syevy\n",
            "nIs hmh n t hFo o,ihCr\n",
            "ina\n",
            " t isoT d r ,ermir \n",
            "o t\n",
            "mnladyhO nSinfd tttht,,dw c\n",
            "   otrnf tnacdbIs,dtIhpiknrg msts\n",
            "SoCt.onTuAmo d.rae daofeelyn wlbnO : rrwm uWyg   oni\n",
            "oioenstel fC  nen n- gmpt sy   r n y\n",
            "iro a I e nT-e rt\n",
            "h eeaoPr? p  he'orrrn:rhgut vgeOs\n",
            "  osmh\n",
            ",ea,\n",
            "ute wIlhuv  dah,tuetn tu A ne g . saI\n",
            "elynrimGl a o:dmyTo  hvnLegnya,nr on tsu\n",
            "Tu\n",
            "dihrBsEU hatii w mHmgelhrro d'a.obtQ' uaT nddfnn kmOb'hC htl t\n",
            "e vEt\n",
            "?yattHrnoHhwv ea a :vTs-wlphuiMct rtlqh prs:nn V, r,iie \n",
            "snrX  tm,i.T tsrs  \n",
            "\n",
            "iey d:.yn \n",
            " asrAo\n",
            " sdtunnou\n",
            ":oaUa, : si agtina etr  r r:aihbbe tdlo   ,vNfnreh re:;dhehwTi\n",
            "d l-nl tcano:r , ,b oo j  :f: rix a l rstrNalraTf t qi mw.et peno nt ripIuih stdo ,es:\n",
            "t s mdrrstunaei sltd dnaos a,\n",
            "\n",
            "jmnnyTladlrn nhm La hu\n",
            " e a:f aeG\n",
            "te tsP  at\n",
            "aIn  d.nH,tm ,pltn h n lf iteboraou\n",
            "hitIisnugnynnttkaanrrihor,rnNhtauaoitn p afnnamdhh ,ggncd,\n",
            "n,   ft  tooio htli I  fg sil d t uit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JThByH8lh5Zl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}